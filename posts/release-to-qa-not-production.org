#+BEGIN_COMMENT
.. title: Don't release to Production, release to QA
.. slug: release-to-qa-not-production
.. date: 2022-05-30 15:55:25 UTC+03:00
.. tags:
.. category:
.. link:
.. description:
.. type: text
.. status: draft
#+END_COMMENT

  *Automate your release workflow to such extent that the QA engineers from your team become the users of the application.*

* Introduction

  It's friday afternoon, the end of the sprint, and a few hours before the weekend starts, and the QA engineers are performing the required tests on the last Sprint Backlog Item (SBI). The developer responsible for that item, confident that the SBI meets all acceptance criteria, is already in the weekend mood.

  Suddenly, a notification pops up --- there is an issue with the feature being tested. The developer jumps on it to see what the problem is, and after discussing with the QA engineer he/she finds out that the issue is caused by some leftover data from the previous SBI.

  Having identified the problem, the developer spends a few minutes to craft a SQL script that will clean the data, and gives it to the QA engineer. The QA engineer runs the script on the QA database, starts testing the SBI from the beginning, and then confirms that the application is "back to normal".

  Both sigh in relief while the SBI is marked as "Done" and the weekend starts. Bliss!

* Getting to the root problem

  Although the day and the sprint goal were saved, I would argue that applying the cleanup script just fixed an issue but left the root problem untouched. And to get to the root problem, let's take a closer look on what happened.

  The database issue stems from the fact that instead of being kept as close as possible to production data (as the best practices suggest) the database grew to become an entity of its own through not being kept tidy by the team.

  When testing a SBI involves changing some of the data from the database, it is not very often that those changes are reverted as soon as the SBI leaves the QA environment. With each such change the two databases (production and QA) grow further and further apart, and the probability of having to apply a workaround script increases each time. However  the paradox is that the cleanup script, although it solves the issue, is /yet another change to the data/ which widens even more the gap between QA and production databases.

  And there lies our problem: not within the workaround script itself, but within the practice of applying workarounds to patch the proverbial broken pipes instead of building actual deployment pipelines.

  But this problem goes one level deeper; sure, we can fix the problem at hand by restoring the database from a production backup but to solve the issue once and for all we need to change how we look at QA environment.

  But our root-cause analysis is not complete yet. We can't just say "let's never apply workarounds" because workarounds are some sort of necessary evil. Let's look at why that is, shall we?


  How to do this? Treat the QA like a user, and enable him/her to do so by alleviating the burden of the plethora of tasks that he/she must do before shifting into the user mindset. And now we move onto how to remove that burden.

* How to treat QA environment like Production?

  Production and QA environments are different (/verry/ different I may add); there's no doubt about that. What makes them different, from the point of view of our topic, is the fact that when a feature is deployed in Production environment, /all/ the prerequisites are known, and all preliminary steps are executed.

  On the other hand, when deploying to QA environment we don't always have this knowledge, nor do we have all the preliminary steps completed at all times. Furthermore, deploying on QA may require additional steps than on Production, e.g.: restoring the database to the last backup from Production, data anonymization etc.

* Automation is key

  To alleviate the difference between successive environments you only need to do one thing, although I must say from the start that achieving that one thing /can be really hard/ --- *automate everything*.

  If a release workflow is properly (read /fully/) automated, then the difference between various environments is reduced mainly to one thing --- the group of people who have proper access rights to run the workflow on the specific environment. With today's' tools on the market the difference becomes simplified further --- it is in the group of people that are allowed to see or to push the "Deploy" button.

* Tight feedback loop

  Once you automate the tasks that require the QA engineer to spend time in a mindset other than the mindset of a user, you have a two-fold gain: (i) faster execution of automated tasks (because computers do boring stuff quicker than humans), which allows for (ii) more time to be spent in the mindset of a user.

  And when it takes less time for a QA engineer to get into the mindset of a user, then it takes (obviously) less time for the defects he/she finds to be reported. We have a name for such a thing --- it's called a /feedback loop/. The feedback loop is not introduced by automation --- it was long present in the project, however, what workflow automation does is that it tightens the feedback loop, which means we have to wait less time in order to get a result from the changes we introduced in our code.

* The Snowball Effect

  One great thing about having a small/tight feedback loop as the one you can get when QA environment is treated as production is that it starts a [[https://en.wikipedia.org/wiki/Snowball_effect][snowball effect]] where, as mentioned before, automation is key.

  At first, you gain efficiency --- there is no checklist to go through when deploying, no time needed to spend doing the tedious steps of deployment; the computers will perform those steps as quickly as possible and /always/ in the same order without skipping any of them or making the errors that humans usually do when performing tedious work. With a click of a button, or on a certain event the deployment starts and while it runs the people from the team are free to do whatever they want in the time it takes to deploy: they can have a cup of coffee, can make small talk with a colleague, or can mind the more important business like the overall quality of the product they're working on.
  Furthermore, besides efficiency you can gain speed --- just by delegating the deployment process to computers you can gain some --- computers do tedious things faster.

  With efficiency and speed comes a reduced what Martin Fowler calls [[https://martinfowler.com/articles/branching-patterns.html#integration-friction][integration friction]].

  Less integration friction means higher integration frequency, more deployments which make the QA engineers work more with the app they're testing. And this is where the magic unravels.

* Conclusion

  Having to see your colleague from QA as a user of the application (and thus whose word weighs a lot in the discussions about the application) is strange. After all, you both know a lot more of what's under the hood of the application for any of you to be considered a simple user of it.

  But if you take from the QA engineer all the hassle of deployment and fiddling with making the application run properly in the testing environment you are freeing his/her time for working with the application just enough time to shift his/her mindset into the mindset of an actual user, and having a user of the application close by is a treasure trove for building the application in such a way that it accomplishes its purpose --- catering to users' needs.

  And, as the saying goes, to change the world you need to start with changing yourself. This change comes when you treat QA environment as production environment and make all the efforts needed to uphold the delivery to QA to the same rigor as delivery to production. In essence, it's nothing but a shift in the mindset that was already mentioned in the title --- don't release to Production, release to QA.
