<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Variational Autoencoders pe înţelesul meu | Rebeja</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://rebeja.eu/posts/vae-tutorial/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Petru Rebeja">
<meta property="og:site_name" content="Rebeja">
<meta property="og:title" content="Variational Autoencoders pe înţelesul meu">
<meta property="og:url" content="https://rebeja.eu/posts/vae-tutorial/">
<meta property="og:description" content="Outline


The core: latent variables and latent space
The probabilistic perspective: \(p(l|w) \rightarrow p(z|w) \wedge p(l|z)\)
The deep learning perspective: ELBO loss and mapping function from z  t">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2020-06-02T23:13:03+03:00">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Header and menu bar -->
<div class="container">
      <header class="blog-header py-3"><div class="row nbb-header align-items-center">
          <div class="col-md-3 col-xs-2 col-sm-2" style="width: auto;">
            <button class="navbar-toggler navbar-light bg-light nbb-navbar-toggler" type="button" data-toggle="collapse" data-target=".bs-nav-collapsible" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse bs-nav-collapsible bootblog4-search-form-holder">
                
            </div>
        </div>
          <div class="col-md-6 col-xs-10 col-sm-10 bootblog4-brand" style="width: auto;">
            <a class="navbar-brand blog-header-logo text-dark" href="https://rebeja.eu/">

            <span id="blog-title">Rebeja</span>
        </a>
          </div>
            <div class="col-md-3 justify-content-end align-items-center bs-nav-collapsible collapse flex-collapse bootblog4-right-nav">
            <nav class="navbar navbar-light bg-white"><ul class="navbar-nav bootblog4-right-nav">
<li class="nav-item">
    <a href="index.org" id="sourcelink" class="nav-link">Source</a>
    </li>


                    
            </ul></nav>
</div>
    </div>
</header><nav class="navbar navbar-expand-md navbar-light bg-white static-top"><div class="collapse navbar-collapse bs-nav-collapsible" id="bs-navbar">
            <ul class="navbar-nav nav-fill d-flex w-100">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="../../rss.xml" class="nav-link">RSS feed</a>
                </li>
<li class="nav-item">
<a href="../../pages/en-ro-dictionary-for-ai" class="nav-link">Dicţionar EN-RO pentru IA</a>

                
            </li>
</ul>
</div>
<!-- /.navbar-collapse -->
</nav>
</div>

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Variational Autoencoders pe înţelesul meu</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Petru Rebeja
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2020-06-02T23:13:03+03:00" itemprop="datePublished" title="2020-06-02 23:13">2020-06-02 23:13</time></a>
            </p>
                <p class="commentline">
        
    <a href="#disqus_thread" data-disqus-identifier="cache/posts/vae-tutorial.html">Comments</a>


            
        </p>
<p class="sourceline"><a href="index.org" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div id="outline-container-org31de29d" class="outline-2">
<h2 id="org31de29d">Outline</h2>
<div class="outline-text-2" id="text-org31de29d">
<ul class="org-ul">
<li>The core: latent variables and latent space</li>
<li>The probabilistic perspective: \(p(l|w) \rightarrow p(z|w) \wedge p(l|z)\)</li>
<li>The deep learning perspective: <code>ELBO loss</code> and mapping function from <code>z</code>  to <code>l</code>.</li>
</ul>
</div>
</div>
<div id="outline-container-orga33a927" class="outline-2">
<h2 id="orga33a927">Prolog</h2>
<div class="outline-text-2" id="text-orga33a927">
<p>
Înainte de a începe discuția despre subiectul propriu-zis al acestui articol vreau să răspund (mai mult pentru mine) la două întrebări:
</p>
<ul class="org-ul">
<li>De ce mai scriu încă un articol despre <code>Variational Autoencoders</code>, pe lângă <i>multitudinea</i> de articole deja existente pe Internet?</li>
</ul>
<p>
Răspunsul este simplu. <code>Variational Autoencoders</code> este un subiect cu care m-am întâlnit din ce în ce mai des în ultimul timp și vreau să-l înțeleg mai bine. Iar cel mai bun mod de a înțelege ceva este să explici acel ceva altor persoane. Din acest motiv m-am decis să: (i) fac o <a href="https://iasi.ai/meetups/the-bridge-between-deep-learning-and-probabilistic-machine-learning/">prezentare despre <code>VAE</code></a> în cadrul Iași AI și (ii) să scriu acest articol pe blog, în limba română, ca material suplimentar.
</p>

<ul class="org-ul">
<li>De ce în limba română?</li>
</ul>
<p>
Pentru că traducând toate noțiunile și conceptele din engleză mă forțez să le înțeleg mai bine. În plus, mai am ocazia să adaug termeni noi la <a href="https://rebeja.eu/pages/en-ro-dictionary-for-ai">lista de traduceri din engleză în română a termenilor din Inteligenţa Artificială și Învățarea Automată</a>.
</p>

<p>
Acestea fiind spuse, să purcedem în aventura noastră în lumea modelelor probabilistice!
</p>

<p>
Și cum orice aventură întinsă pe un fir epic începe cu o introducere, înainte de a discuta despre ce este un <code>Variational Autoencoder</code> trebuie să vorbim mai întâi despre ce este un <code>Autoencoder</code> pentru ca, având imaginea unui <code>autoencoder</code> în minte să vedem mai apoi că, deși sunt similare ca nume, comparat cu un <code>autoencoder</code> clasic un <code>variational autoencoder</code> este cu totul altă mâncare de peşte.
</p>

<p>
Așadar, începem cu:
</p>
</div>
</div>
<div id="outline-container-orgd34b9e0" class="outline-2">
<h2 id="orgd34b9e0">Ce este un <code>Autoencoder</code>?</h2>
<div class="outline-text-2" id="text-orgd34b9e0">
<p>
În linii mari, un <code>autoencoder</code> este o rețea neuronală alcătuită din două parți: <i>codor</i> și <i>decodor</i> (<code>encoder</code> și respectiv <code>decoder</code>), a cărei funcționalitate este să:
</p>
<ol class="org-ol">
<li>primească la intrare un vector \(X \in R^D\)</li>
<li>îl codifice (<code>encode</code>) într-un vector \(z \in R^K\) unde \(K \ll D\). În termeni matematici, codorul învață o funcție \(g:R^D \rightarrow R^K\)</li>
<li>decodifice vectorul \(z\) în-un vector \(X'\) cât mai similar posibil cu vectorul original \(X\). Altfel spus, decodorul învață funcția \(f:R^K \rightarrow R^D\) aşa încât \(f(g(X)) \approx X\). <a id="orgbbce13a"></a>
</li>
</ol>
<p>
De ce ne interesează acest mecanism? De exemplu, pentru identificarea anomaliilor<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup><sup>, </sup><sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>.
</p>

<p>
Aşa cum am spus mai sus, pentru vectori \(X\) asemănători cu cei întâlniți în setul de antrenament, codorul va învăța să producă o codificare potrivită acestora iar decodorul, la rândul său, va învață să genereze pe baza codificării primite alți vectori ( \(X', X'', \ldots\) ) asemănători cu \(X\). Însă dacă \(X\) este o valoare extremă (<code>outlier</code>) atunci decodorul nu va putea să reconstruiască vectorul inițial \(X\) din reprezentarea acestuia \(z\), adică \(f(g(X)) \neq X\).
</p>

<p>
Cu alte cuvinte, <i>cu cât rezultatul \(X'\) este mai diferit de vectorul primit la intrare \(X\), cu atât este mai probabil ca \(X\) să fie o valoare extremă</i>.
</p>

<p>
Într-o singură imagine, arhitectura unui <code>autoencoder</code> arată așa:
<img src="../../images/autoencoder-schema.png" alt="nil"></p>

<p>
Am desenat nodul \(z\) cu linie întreruptă pentru că în practică el nu se mai include în model ci se conectează codorul direct la decodor.
</p>

<p>
Având această imagine în minte, putem să începem discuția despre subiectul de interes pentru acest articol.
</p>
</div>
</div>
<div id="outline-container-orgc6274d8" class="outline-2">
<h2 id="orgc6274d8">Ce este un <code>Variational Autoencoder</code>?</h2>
<div class="outline-text-2" id="text-orgc6274d8">
<p>
Privind de la un nivel înalt, un <code>variational autoencoder</code><sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup><sup>, </sup><sup><a id="fnr.4" class="footref" href="#fn.4">4</a></sup>, pe numele său mic <code>VAE</code>, are aceeași arhitectură ca un <code>autoencoder</code>. Astfel și într-un <code>VAE</code> regăsim componentele binecunoscute ale unui <code>autoencoder</code>:
</p>
<ul class="org-ul">
<li>un codor care primește la intrare un \(X\) și îl transformă în reprezentarea latentă \(z\),</li>
<li>un decodor care, având reprezentarea latentă \(z\), învață să reconstruiască vectorul original \(X\) din \(z\).</li>
</ul>
<p>
Dar, aşa cum spune englezescul „The devil is in the detail”, la fel și în cazul <code>VAE</code> detaliile sunt cele care fac asemănările dintre cele două arhitecturi să se termine exact aici: la nume și la cele două componente; pentru că, pe măsură ce vom intra în detalii vom vedea cât de diferite sunt componentele de la o arhitectură la alta.
</p>

<p>
Și ca să justific întrebuințarea proverbului de mai sus am introdus deja un detaliu subtil menit să sublinieze diferența dintre cele două arhitecturi. Este vorba despre vectorul \(z\) care în cazul unui <code>autoencoder</code> este <i>o codificare</i> a lui \(X\) iar în cazul unui <code>VAE</code> este <i>reprezentarea latentă</i> a lui \(X\).
</p>

<p>
  Care-i diferența dintre <i>codificare</i> și <i>reprezentare latentă</i>?
Pe scurt, codificarea este o transformare iar reprezentarea latentă este un amestec al însușirilor/atributelor primordiale ale variabilei \(X\). O altă diferență ar fi următoarea: codificarea este construită iar reprezentarea latentă identificată și/sau dedusă.
</p>

<p>
În acest sens, un <code>autoencoder</code> poate fi asociat cu o aplicație de comprimare/decomprimare care învață singură algoritmii necesari pentru cea mai eficientă comprimare. Astfel, vectorul \(X\) este transformat în vectorul \(z\) astfel încât \(z\) să conțină suficientă informație pentru reconstruirea lui \(X\) inițial.
</p>

<p>
Un <code>VAE</code> însă, dată fiind natura sa generativă, trebuie asociat cu o aplicație care primește la intrare un fișier și pe baza caracteristicilor acestuia generează fișiere asemănătoare care diferă între ele prin mici variații.
</p>

<p>
Care sunt caracteristicile unui fișier? Depinde de fișier. Pentru o imagine ar fi paleta de culori, orientarea, tipul de obiecte surprinse de camera foto și multe altele. Pentru un text acestea ar putea fi lungimea, prezența sau absența figurilor de stil, particularitățile de scriere etc. Totalitatea acestor caracteristici ascunse însumează <i>reprezentarea latentă</i>.
</p>

<p>
Evident, nu este posibil să specificăm manual aceste caracteristici de fiecare dată, iar acolo unde este posibil, costurile aferente acestui proces ar fi enorme, fie că vorbim de timp, bani, resurse umane sau oricare combinație a acestora. Aici <code>VAE</code> vine în ajutorul nostru preluând sarcina de a deduce reprezentările latente pentru intrările primite în stadiul de antrenare.
</p>

<p>
Prezența acestei părți de deducție a reprezentărilor latente merită o mențiune aparte: <b><code>VAE</code> nu este un model generativ</b><sup><a id="fnr.5" class="footref" href="#fn.5">5</a></sup>. <b>Modelul generativ face parte din <code>VAE</code></b>, mai exact, acesta este decodorul menționat la începutul acestei secțiuni dar <code>VAE</code> este mai mult decât atât.
</p>

<p>
Acestea fiind spuse, <b>definim un <code>Variational Autoencoder</code> ca fiind un <i>ansamblu</i> care constă dintr-o rețea de inferență și un model generativ</b> <sup><a id="fnr.5.100" class="footref" href="#fn.5">5</a></sup>.
</p>

<p>
Definiția de mai sus, pentru mine, a fost puntea de trecere dintre învățarea automată aprofundată (<code>deep learning</code>) și învățarea automată probabilistică, ambele domenii regăsindu-se în ansamblul <code>VAE</code>. O dată ce am pășit pe acest mal metaforic al învățării automate probabilistice am descoperit o lume nouă; o lume cel puțin la fel de interesantă ca lumea celuilalt mal — cel al rețelelor neuronale și învățării automate aprofundate.
</p>

<p>
Un <code>VAE</code> trebuie explicat din ambele puncte de vedere, așa cum a făcut Jaan Altosaar în articolul<sup><a id="fnr.6" class="footref" href="#fn.6">6</a></sup> său, deoarece privit dintr-o singură perspectivă, imaginea a ceea ce este cu adevărat un <code>VAE</code> este neclară și/sau incompletă, așa cum s-a văzut mai sus când am menționat că un <code>VAE</code> este mai mult decât un simplu model generativ.
</p>

<p>
Abia văzut din ambele perspective, modelul <code>VAE</code> ni se arată în toată splendoarea sa, ceea ce vom și face în continuare.
</p>
</div>
</div>
<div id="outline-container-org79bbe46" class="outline-2">
<h2 id="org79bbe46">
<code>VAE</code> din perspectiva <code>Deep Learning</code>
</h2>
<div class="outline-text-2" id="text-org79bbe46">
<p>
Din perspectiva învățării automate aprofundate, <code>VAE</code> poate fi asociat cu un autoencoder care a suferit următoarele modificări:
</p>
<ul class="org-ul">
<li>are o arhitectură mai complicată,</li>
<li>are funcția de cost (<code>loss function</code>) mai ciudată,</li>
<li>are două noduri de intrare, dintre care unul primește numere aleatorii,</li>
<li>aplică două artificii de calcul:
<ol class="org-ol">
<li>Se optimizează o funcție negată (despre asta mai târziu),</li>
<li>Funcția decodor primește ca parametru un număr aleator pentru a putea fi considerată derivabilă (și implicit să poată fi învățată).</li>
</ol>
</li>
</ul>
<p>
Și cum o imagine face cât o mie de cuvinte, arhitectura de nivel înalt a unui <code>Variational Autoencoder</code> arată așa:
<a id="org8b127fa"></a>
</p>
<p>
<img src="../../images/vae-schema.png" alt="nil"></p>

<p>
Pornind de la diagramele arhitecturale ale celor două modele, începem să identificăm și diferențele, din ce în ce mai evidente dintre un autoencoder clasic și un <code>VAE</code>. Din diagrama de mai sus observăm că, spre deosebire de un autoencoder clasic, un <code>VAE</code> nu învață să identifice direct reprezentarea codificată a lui \(X\) — \(z\). Modelul de fapt învață următoarele:
</p>
<dl class="org-dl">
<dt>Codorul</dt>
<dd>Învață să identifice parametrii care descriu distribuția statistică a reprezentărilor latente. Cu alte cuvinte, <b>codorul</b> nu identifică o reprezentare directă a lui \(X\) ci <b>identifică distribuția statistică a reprezentărilor lui \(X\)</b>. Intuiția din spatele acestui comportament este că dacă modelul va învăța să genereze date asemănătoare celor din setul de antrenament atunci este foarte probabil să genereze date asemănătoare și pentru celelalte date<sup><a id="fnr.7" class="footref" href="#fn.7">7</a></sup>. Cum o distribuție poate fi descrisă prin media (\(\mu\)) și deviația standard (\(\sigma\)), aceștia sunt parametrii identificați de codor.</dd>
<dt>Decodorul</dt>
<dd>Învață de fapt două funcții:
<ol class="org-ol">
<li>O funcție care transformă un punct din distribuția dată de \((\mu, \sigma)\) într-un punct din spațiul <i>reprezentărilor latente ale lui \(X\)</i>. Cum face asta în cazul <code>VAE</code>? Simplu: învață o funcție care aranjează punctele unei distribuții date de \((\mu, \sigma)\) în forma necesară pentru \(z\)<sup><a id="fnr.7.100" class="footref" href="#fn.7">7</a></sup>.</li>
<li>O funcție care transformă reprezentarea latentă \(z\) în instanța primită la intrare.</li>
</ol>
</dd>
</dl>
<p>
Așadar, schema din <a href="#org8b127fa">imaginea anterioară</a> este simplificată. Adevărata arhitectură a unui <code>VAE</code> arată astfel:
  <img src="../../images/vae-schema-complete.png" alt="nil"></p>

<p>
La fel ca în diagrama pentru <code>autoencoder</code>, reprezentarea latentă \(z\) este desenată cu linie întreruptă deoarece în practică ea nu se regăsește în model.
</p>

<p>
Abia această ultimă diagramă, cea cu arhitectura completă, ne oferă imaginea clară a diferențelor dintre un <code>autoencoder</code> clasic și un <code>VAE</code>: în cazul <code>VAE</code> nu mai vorbim de o compunere de funcții cum am văzut în cazul unui <a href="#orgbbce13a">autoencoder</a>. În principiu vorbim despre două <i>componente diferite</i>: (i) <b>rețeaua de inferență</b> care ne permite să aproximăm parametrii \((\mu, \sigma)\) și (ii) <b>modelul</b> propriu-zis care generează instanțe noi.
</p>

<p>
Și cum orice discuție despre un model neuronal trebuie să includă și funcția de cost, iată și funcția pe care o utilizează un <code>VAE</code> pentru fiecare punct \(x_i\):
\[
  l_i(\theta, \phi) = -E_{z \sim q_{\theta}(z|x_i)}[log_{p_{\phi}}(x_i|z)] + KL(q_{\theta}(z|x_i) || p(z))
  \]
Da, această formulă este lungă ca o zi de post, dar interpretarea ei ne spune mai exact pe ce trebuie să pună accentul modelul în stadiul de antrenament.
</p>

<p>
Prima parte, \(E_{z \sim q_{\theta}(z|x_i)}[log_{p_{\phi}}(x_i|z)]\), reprezintă <i>pierderea de informație la reconstruirea lui \(X\)</i>. Acest termen obligă modelul să construiască o instanță nouă \(x_i'\) care să fie foarte asemănătoare cu instanța \(x_i\) primită la intrare. Implicit, cât \(x_i'\) va fi mai diferit de \(x_i\) cu atât penalizarea va fi mai mare pentru model.
</p>



<p>
După cum se observă deja din funcția de cost, am trecut de jumătatea podului și deci trebuie să abordăm:
</p>
</div>
</div>
<div id="outline-container-orgebccd6a" class="outline-2">
<h2 id="orgebccd6a">
<code>VAE</code> din perspectiva probabilistică</h2>
<div class="outline-text-2" id="text-orgebccd6a">
<p>
<b>Rețeaua de inferență</b>
</p>

<p>
O reţea de inferenţă este o reţea neuronală care permite să identificăm parametrii optimi pentru aproximarea unor distribuţii<sup><a id="fnr.8" class="footref" href="#fn.8">8</a></sup>. Avantajul unei astfel de reţele este că putem generaliza identificarea parametrilor \(\mu\) şi \(\sigma\) atât pentru punctele din setul de date de antrenament cât şi pentru punctele nemaîntâlnite până acum.
</p>
</div>
</div>
<div id="outline-container-orga542545" class="outline-2">
<h2 id="orga542545">Cum funcționează?</h2>
</div>
<div id="outline-container-org05ded4c" class="outline-2">
<h2 id="org05ded4c">O implementare elegantă<sup><a id="fnr.9" class="footref" href="#fn.9">9</a></sup>
</h2>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef">
<sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup><div class="footpara"><p class="footpara">
<a href="https://dl.acm.org/doi/10.1145/2689746.2689747">Anomaly Detection Using Autoencoders with Nonlinear Dimensionality Reduction</a>
</p></div>
</div>

<div class="footdef">
<sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup><div class="footpara"><p class="footpara">
<a href="https://dl.acm.org/doi/abs/10.1145/3097983.3098052">Anomaly Detection with Robust Deep Autoencoders</a>
</p></div>
</div>

<div class="footdef">
<sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup><div class="footpara"><p class="footpara">
<a href="https://arxiv.org/abs/1312.6114">Kingma, D. P. and Welling M., (2014) Auto-Encoding Variational Bayes</a>
</p></div>
</div>

<div class="footdef">
<sup><a id="fn.4" class="footnum" href="#fnr.4">4</a></sup><div class="footpara"><p class="footpara">
<a href="https://arxiv.org/abs/1401.4082">Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models</a>
</p></div>
</div>

<div class="footdef">
<sup><a id="fn.5" class="footnum" href="#fnr.5">5</a></sup><div class="footpara"><p class="footpara">
<a href="http://dustintran.com/blog/variational-auto-encoders-do-not-train-complex-generative-models">Variational auto-encoders do not train complex generative models | Dustin Tran</a>
</p></div>
</div>

<div class="footdef">
<sup><a id="fn.6" class="footnum" href="#fnr.6">6</a></sup><div class="footpara"><p class="footpara">
<a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">Tutorial - What is a variational autoencoder? – Jaan Altosaar</a>
</p></div>
</div>

<div class="footdef">
<sup><a id="fn.7" class="footnum" href="#fnr.7">7</a></sup><div class="footpara"><p class="footpara">
<a href="https://arxiv.org/abs/1606.05908">Doersch, C., (2016) Tutorial on variational autoencoders</a>
</p></div>
</div>

<div class="footdef">
<sup><a id="fn.8" class="footnum" href="#fnr.8">8</a></sup><div class="footpara"><p class="footpara">
<a href="http://edwardlib.org/tutorials/inference-networks">Edward – Inference Networks</a>
</p></div>
</div>

<div class="footdef">
<sup><a id="fn.9" class="footnum" href="#fnr.9">9</a></sup><div class="footpara"><p class="footpara">
<a href="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial">Implementing Variational Autoencoders in Keras: Beyond the Quickstart Tutorial</a>
</p></div>
</div>


</div>
</div>
    </div>
    <aside class="postpromonav"><nav></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
        
        
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="rebeja",
            disqus_url="https://rebeja.eu/posts/vae-tutorial/",
        disqus_title="Variational Autoencoders pe \u00een\u0163elesul meu",
        disqus_identifier="cache/posts/vae-tutorial.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
        </script></article><script>var disqus_shortname="rebeja";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><!--End of body content--><footer id="footer">
            Contents © 2020         <a href="mailto:petru.rebeja@gmail.com">Petru Rebeja</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
